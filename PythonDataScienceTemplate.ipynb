{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkUhizM3PSOIhy2RrBmwj+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ],
      "metadata": {
        "id": "-PzjPAsl0xm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Modeling with Python and Pandas\n",
        "\n",
        "If this is your first work with Python, **STOP**. Before starting in this notebook you should first review [DataSciencePythonCrISPDM.ipynb](https://colab.research.google.com/drive/1cezizAGahyGMFobMU96Jwxjxluut0xIW?usp=drive_link)\n",
        "\n",
        "Author is Michael McCarthy (mbmccart@utica.edu)\n",
        "\n",
        "Feedback Welcomed"
      ],
      "metadata": {
        "id": "OpQdYOFs0rff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libarary Loading"
      ],
      "metadata": {
        "id": "IsKKwUgt2R2E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JMF6jYsVwsR"
      },
      "source": [
        "# Python interactive development enviroments (IDE) come with base Python 3.x but\n",
        "#     certain modules, packages, and libraries as need.\n",
        "\n",
        "# Pandas is the main way we will work with the dataframes\n",
        "# https://pandas.pydata.org/docs/getting_started/index.html#getting-started\n",
        "import pandas as pd\n",
        "# removes the scientific notation that can be used in outputs with Pandas\n",
        "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
        "# pandas defaults to not showing all rows, this pd (pandas) option update ensures all rows are shown\n",
        "# for larger datasets, update 'None' with a specific number\n",
        "pd.set_option('display.max_rows', None)\n",
        "# pandas defaults to not showing all columns, this pd (pandas) option update ensures all columns are shown\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# MatPlotLib is a common visulaization package.\n",
        "# note that just pyplot is added in, not the full package\n",
        "from matplotlib import pyplot as plt\n",
        "#It is a function that renders the figure in a notebook (instead of displaying a dump of the figure object).\n",
        "%matplotlib inline\n",
        "\n",
        "#numpy is the \"The fundamental package for scientific computing with Python\"\n",
        "# https://numpy.org/\n",
        "import numpy as np\n",
        "# To make outputs more understandable, remove the scientific notation\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "#Setting Seed for reproducable results (important to have tensorflow random seed set as well)\n",
        "#If the randome seed is not set, then some models will have different results each time (a very frustrating thing)\n",
        "#https://datascience.stackexchange.com/questions/13314/causes-of-inconsistent-results-with-neural-network\n",
        "np.random.seed(1)\n",
        "\n",
        "# package for descriptive statistics, there are others you can use like seaborne\n",
        "!pip install researchpy\n",
        "import researchpy as rp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOh3OwyWelW"
      },
      "source": [
        "# Packages & Libraries needed to load data from Google Drive\n",
        "# For this class, ALL Data will be loaded from Google Drive.\n",
        "# TIP: Load data just once.\n",
        "# https://pypi.org/project/PyDrive2/\n",
        "\n",
        "!pip install -U -q PyDrive2\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siFOuBXWWh7v"
      },
      "source": [
        "# Authenticate users to have acces to google Drive.\n",
        "# Google will make you authorize access to connect directly to the Google Drive\n",
        "# The process might change, just approve the access by approving or clicking \"Continue\".\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "SPc6WOLudFwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Steps to load your CSV data\n",
        "\n",
        "# 1) Open the Google Drive with the Data\n",
        "# 2) Find your assigned dataset\n",
        "# 3) Click \"share\"\n",
        "# 4) Copy link\n",
        "# 5) Paste link here:\n",
        "#    The link should look something like this: https://drive.google.com/file/d/1WVluSCNJ--RS1zqQ_0EJScPgurw9CmHj/view?usp=sharing\n",
        "# 6) Copy the unique file id, for the example above, it looks like this: 1WVluSCNJ--RS1zqQ_0EJScPgurw9CmHj\n",
        "#   Hint: the file id is all the content between the forward slashes slashes /  including the letter, numbers, dashes, and underscores\n",
        "# 7) In the next code line below, replace the unique google doc file id for the example data with the unique file id for your data.\n",
        "file_id = '1WVluSCNJ--RS1zqQ_0EJScPgurw9CmHj' # replace the id with id of file you want to access\n",
        "downloaded = drive.CreateFile({'id':file_id})\n",
        "\n",
        "# 8) Update the file name below to match the file name in the Google Drive Folder by replacing 'Heart_Synthetic.csv' with your file name.\n",
        "# Hint, you must have single or double quotes around the file name.\n",
        "file_name = 'Heart_Synthetic.csv' # Update needed here, replace the file name with the id of file you want\n",
        "downloaded.GetContentFile(file_name)\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# 9) Run all cells before this code cell (Hint: Shortcut = CTRL + F8)\n",
        "# 10) Run this cell (click the play button or Shift + Enter)\n",
        "# 11) Check the output is what you expected\n",
        "print(f\"{file_name} Data Shape: \",df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "ThHAIpQV1F3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "PKWbj3uc8G2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the data is loaded, we need to understand it.\n",
        "\n",
        "This is the \"Data Understanding\" portion of the *Cross-Industry Standard process for Data Mining* (CrISP-DM)"
      ],
      "metadata": {
        "id": "M5U5rHOA8g8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the descriptive statistics into a dataframe called \"descriptive_statistics_view\"\n",
        "descriptive_statistics_view=df.describe()"
      ],
      "metadata": {
        "id": "s5r3Uzdv73Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the descriptive_statistics_view\n",
        "descriptive_statistics_view"
      ],
      "metadata": {
        "id": "eGaJK-NYq52F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice how the view changes using a print statement\n",
        "print(descriptive_statistics_view)"
      ],
      "metadata": {
        "id": "kt1bN-t8rT82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: build histograms for all numerical values in df\n",
        "\n",
        "# Iterate over numerical columns and create histograms\n",
        "for col in df.select_dtypes(include=np.number):\n",
        "  plt.figure()  # Create a new figure for each histogram\n",
        "  plt.hist(df[col], bins=10)  # Adjust the number of bins as needed\n",
        "  plt.title(f'Histogram of {col}')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "rQ2dvGxB8Nlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrangling"
      ],
      "metadata": {
        "id": "ntjuCGqVc9R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data can be very \"dirty\" or not in the form we need it to be so we do considerable data wrangling.\n",
        "\n",
        "This is the \"Data Preparation\" step in the CriSP-DM."
      ],
      "metadata": {
        "id": "vfpSdUnRA5XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: review df for a list of multiple variable names from a list and drops the variables from the df. the defualt list is just one variable 'RecordID'\n",
        "# NOTE, this was my third prompt. The first two were so vague that Gemini had a **very** long function.\n",
        "\"\"\"\n",
        "Generative AI code will often build a function. This is great because it is reusable within the notebook or can be brought over to another.\n",
        "It is important that the function is run. That is best done in a seperate cell after the function is defined.\n",
        "\"\"\"\n",
        "def drop_variables(df, variables_to_drop=['RecordID']):\n",
        "    \"\"\"\n",
        "    Reviews a Pandas DataFrame for a list of variable names and drops them.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        variables_to_drop: A list of variable names to drop. Defaults to ['RecordID'].\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with the specified variables removed, or the original DataFrame if no variables are found.\n",
        "        Prints a message indicating which variables were dropped or if none were found.\n",
        "    \"\"\"\n",
        "\n",
        "    variables_dropped = []             # defines an empty list to add dropped variables to\n",
        "    for var in variables_to_drop:      # for loop to look at each varible in the list\n",
        "        if var in df.columns:          # if test to look if the variable name is in the dataframe's columns\n",
        "            df = df.drop(var, axis=1)  # drop the variable from the dataframe\n",
        "            variables_dropped.append(var)  # add the variable to the list of dropped variables\n",
        "\n",
        "    if variables_dropped:   # if test defaults to \"TRUE\" so if the list is not blank, it will do the rest of the if statement\n",
        "        print(f\"Variables dropped: {variables_dropped}\")\n",
        "    else:                   # else statements are optional, but should be used\n",
        "        print(\"No variables to drop found in the DataFrame.\")\n",
        "\n",
        "    # Final Notes:\n",
        "    \"\"\"\n",
        "    Gemini did not identify the much simplier way to do this.\n",
        "    However, this one line of code does not build or report the variables_dropped list.\n",
        "    \"\"\"\n",
        "    # df = df.drop(variables_to_drop, errors= 'ignore', axis=1)\n",
        "    \"\"\" This is how it would look in a production enviornment, to prvent the extra memory needed to overwrite the df. \"\"\"\n",
        "    # df.drop(columns=variables_to_drop, errors='ignore', inplace=True) # `axis=1` indicates columns and removed because it is the default, therefore not needed\n",
        "\n",
        "    return df               # identifies that the fucntion returns the original df modified\n"
      ],
      "metadata": {
        "id": "ne6B8p_dAvNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the fution just defined\n",
        "# update the variables in the list to all numerical data that acts as nominal variables\n",
        "df = drop_variables(df, variables_to_drop=['RecordID', 'SSN']) # SSN was added to remove list due to privacy concerns"
      ],
      "metadata": {
        "id": "V07-pWJsCC4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assess the Data types. Make sure we have numbers, not just strings.\n",
        "# Pandas often does a great job of guessing the correct data type, but not always.\n",
        "# Dirty data can cause Pandas to type the feature incorrectly.\n",
        "print(\"\\nTraining DATA\\n\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "NlINKSX82ktW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# descriptive statistics\n",
        "# understanding the descriptive statistics for the full dataset will help use set a benchmark for future comparision\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "-UsIn64h3TSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the dataframe to show the variables and open Interactive table in Colab\n",
        "# NOTE, if you scroll all the way to the right in Colab,\n",
        "#    click the Table icon to view an \"interactive sheet\" that acts like an excel spreadsheet.\n",
        "# The plot icon will generate many suggested plots, only some of them are worth using in your analysis.\n",
        "df\n",
        "# After you you run and view your df, you should see the \"Next steps:\" options under the datframe output. These are wise to use for the very first steps of your EDA.\n",
        "# After you you \"View recommended plots\", be sure to dig deeper with your own plots"
      ],
      "metadata": {
        "id": "FQhsPck6Epq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# matplotlib"
      ],
      "metadata": {
        "id": "v21LcPZ0wRKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: build a histogram for all numerical variables in the df. set the number of bins to the number of unique values in the variable\n",
        "\n",
        "# import pandas as pd # Gemini will include the packages you need even though you already imported these in the first code cell.\n",
        "# it makes your notebook tidy to remove redundant code. I commented out this code, but deleting it is more appropriate.\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# Assuming 'df' is already loaded as in your provided code\n",
        "\n",
        "for col in df.select_dtypes(include=['number']):\n",
        "  num_unique = df[col].nunique()\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.hist(df[col], bins=num_unique)\n",
        "  plt.title(f'Histogram of {col}')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "cBirCYaFHNuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# seaborn"
      ],
      "metadata": {
        "id": "rx1t84ubHL46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need the seaborn library loaded.\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "6Mfm6b2lHR1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the same histogram with seaborn library\n",
        "#you have lots of options\n",
        "sns.histplot(data=df, x='ca')\n",
        "# This variable is unique to the example dataset in \"Heart_Synthetic.csv\", update, move, or delete this cell or it will error because\n",
        "# the variables are not in the new dataset (i.e., dataframe or df)"
      ],
      "metadata": {
        "id": "G7Y2Zx4-HWXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://seaborn.pydata.org/generated/seaborn.catplot.html\n",
        "sns.catplot(data=df, x=\"THAL_string\", y=\"ca\")\n",
        "# This variable is unique to the example dataset in \"Heart_Synthetic.csv\", update, move, or delete this cell or it will error because\n",
        "# the variable is not in the new dataset (i.e., dataframe or df)"
      ],
      "metadata": {
        "id": "wHFBXGSzJYoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#same data, but with a violin plot\n",
        "#https://seaborn.pydata.org/generated/seaborn.violinplot.html#seaborn.violinplot\n",
        "sns.violinplot(data=df, x=\"THAL_string\", y=\"ca\")"
      ],
      "metadata": {
        "id": "Veo0L4eDJ_yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#swarm Plot\n",
        "#https://seaborn.pydata.org/generated/seaborn.swarmplot.html#seaborn.swarmplot\n",
        "sns.catplot(data=df, x=\"THAL_string\", y=\"ca\", hue=\"SEX_string\", kind=\"swarm\")\n"
      ],
      "metadata": {
        "id": "WzxjY5vKKSPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Box & Whisker\n",
        "#https://seaborn.pydata.org/generated/seaborn.boxplot.html#seaborn.boxplot\n",
        "sns.boxplot(data=df, x=\"THAL_string\", y=\"ca\", hue=\"SEX_string\")"
      ],
      "metadata": {
        "id": "UZoxihHDK6YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: insert sns correlation matrix\n",
        "\n",
        "# Correlation Matrix\n",
        "# https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "correlation_matrix = df.select_dtypes(include=np.number).corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T82Re6YcDwua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another way to use sns to view correlations\n",
        "sns.pairplot(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MLp_wvcjCT-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a loop to identify all numerical values in the df and then perform a boxplot and a histogram. Be sure to label each plot.\n",
        "\n",
        "# Loop through columns to identify numerical features\n",
        "for col in df.columns:\n",
        "  if pd.api.types.is_numeric_dtype(df[col]):\n",
        "    # Create a boxplot\n",
        "    plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.show()\n",
        "\n",
        "    # Create a histogram\n",
        "    plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "    sns.histplot(df[col])\n",
        "    plt.title(f'Histogram of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0ma-FxaGLNgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plotly"
      ],
      "metadata": {
        "id": "0_wIphIaHnig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at the Dependent Variable as it relates to these categorical variables\n",
        "#consider this type of side-by-side box and whisker\n",
        "#https://plotly.com/python/plotly-express/\n",
        "import plotly.express as px\n",
        "px.box(data_frame=df,x='SEX_string', y='incident')\n"
      ],
      "metadata": {
        "id": "I1rqzxj66EIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.box(data_frame=df,x='Race_String', y='incident')\n",
        "# does the plot show outliers????"
      ],
      "metadata": {
        "id": "7GPeS9J5INSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data prep for modeling"
      ],
      "metadata": {
        "id": "4igAPa_xaaFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assess the shape before get dummies\n",
        "df.shape"
      ],
      "metadata": {
        "id": "UR3x9xDgcctF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load variable name to paste into get dummies below\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "REI2qQ10a973"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing nulls and nans"
      ],
      "metadata": {
        "id": "w_4oAjz68Q7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: build a new dataframe called \"no_nulls_df\" that will apply the mean to each null in the numerical variables and the mode to each null in the categorical variables.\n",
        "\n",
        "# Create a copy of the DataFrame to avoid modifying the original\n",
        "no_nulls_df = df.copy()\n",
        "\n",
        "# Fill nulls in numerical columns with the mean\n",
        "numerical_cols = no_nulls_df.select_dtypes(include=np.number).columns\n",
        "for col in numerical_cols:\n",
        "    no_nulls_df[col] = no_nulls_df[col].fillna(no_nulls_df[col].mean())\n",
        "\n",
        "# Fill nulls in categorical columns with the mode\n",
        "categorical_cols = no_nulls_df.select_dtypes(exclude=np.number).columns\n",
        "for col in categorical_cols:\n",
        "    no_nulls_df[col] = no_nulls_df[col].fillna(no_nulls_df[col].mode()[0])\n"
      ],
      "metadata": {
        "id": "PVCsMOc1sfRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the synthetic heart data, note that the dfshape with get_dummies went from 17 columns to 32 columns\n",
        "no_nulls_df.shape"
      ],
      "metadata": {
        "id": "cQVhjrCWbj6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing Categorical Variables"
      ],
      "metadata": {
        "id": "JEbwuxNi8KqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*sklearn* will not model with categorical variables.\n",
        "\n",
        "*pandas* get_dummies transform all categorical variables into a boolean (True or False). A boolean is still a catgorical variable so not helpful for sklearn.\n",
        "\n",
        "*sklearn* has its own encoding tools that transform variables into 0 representing  \"False\" and 1 representing \"True\". This is the better option."
      ],
      "metadata": {
        "id": "nvbFvt608Ygm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn for one hot encoding and test train split\n",
        "# https://www.freecodecamp.org/news/how-to-build-and-train-linear-and-logistic-regression-ml-models-in-python/\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "IoayUBHpJeYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some categorical variables need to be transformed into numbers via one-hot encoding or get dummies.\n",
        "# This is very important to do BEFORE splitting data into Testing and Training\n",
        "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
        "\n",
        "# To run this, remove the '#' from the next line\n",
        "# df= pd.get_dummies(data=df)\n",
        "\n",
        "# NOTE the use of df for the dataframe name with get dummies will update the df for now on.\n",
        "# If I want a unique data frame that is different from the orginal dataframe, I need a different name like 'df_model'\n",
        "#,columns=[\"SEX_string\", \"CP_string\", \"RESTECG_string\",\"EXANG_string\",\"FBS_string\",\"SLOPE_string\",\"THAL_string\",\"Race_String\"]"
      ],
      "metadata": {
        "id": "WjtRqh8ham4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "# interesting article about why one-hot encoding (ohe) is better in ML\n",
        "# https://albertum.medium.com/preprocessing-onehotencoder-vs-pandas-get-dummies-3de1f3d77dcc\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Make a list of the columns to one-hot encode\n",
        "columns_to_encode = ['CP_string','RESTECG_string']\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "for col in columns_to_encode:\n",
        "  # Fit and transform the selected column\n",
        "  transformed_data = ohe.fit_transform(df[[col]])\n",
        "\n",
        "  # Create a DataFrame from the transformed data with appropriate column names\n",
        "  ohe_df = pd.DataFrame(transformed_data, columns=ohe.get_feature_names_out([col]))\n",
        "\n",
        "  # Concatenate the new DataFrame with the original DataFrame and drop the original column\n",
        "  df = pd.concat([df.drop(col, axis=1), ohe_df], axis=1)\n",
        "\n",
        "  print(f\"One-hot encoded '{col}' and added to the DataFrame.\")\n",
        "  print(\"Updated DataFrame shape:\", df.shape)"
      ],
      "metadata": {
        "id": "c0g3G7pqbArS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # For initial modeling with sklearn, we remove categorical variables from no_nulls_df making num_no_nulls_df\n",
        "# prompt: build a function to remove all non numerial variables from the a df and report the specific variables that are removed\n",
        "\n",
        "def drop_non_numerical_cols(df):\n",
        "    \"\"\"\n",
        "    Removes non-numerical columns from a Pandas DataFrame and reports the removed columns.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - The DataFrame with non-numerical columns removed.\n",
        "            - A list of the names of the removed columns.\n",
        "    \"\"\"\n",
        "    numerical_df = df.select_dtypes(include=np.number)\n",
        "    removed_cols = list(set(df.columns) - set(numerical_df.columns))\n",
        "    print(\"\\nRemoved columns: \")\n",
        "    for col in removed_cols:\n",
        "        print(col)\n",
        "    return numerical_df, removed_cols\n"
      ],
      "metadata": {
        "id": "ecG5Udmo4rzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For initial modeling with sklearn, we remove categorical variables from no_nulls_df making num_no_nulls_df\n",
        "num_no_nulls_df, list_categorical_cols_removed = drop_non_numerical_cols(no_nulls_df)\n"
      ],
      "metadata": {
        "id": "a7STA2Ab9HeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if categorical variables are encoded, we need to use drop variables that are reporting the same data in different ways,\n",
        "# EX: dichotmous variables from get_dummies like \"SEX_string_male\" and \"SEX_string_female\"\n",
        "# del used above to remove one variable, but df.drop used here to remove multiple columns in one go\n",
        "\n",
        "no_nulls_df.drop(['insert_list_of_cat_vars_here_01','insert_list_of_cat_vars_here_02'],\n",
        "        axis=1, #axis 1 means columns, the drop tool can work on rows if axis=0 . . . which is the default\n",
        "        inplace=True,\n",
        "        errors=\"ignore\") # if a variable in the list is not in the df, then it will not error\n",
        " # We only want numerical data for the linear regression.\n",
        "  # Nominal data must be transformed using `get_dummies` or ohe.\n",
        "  # Ordinal data can be transformed with get_dummies/ohe OR an ordinalEncoder\n",
        "  # https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
        " # all \"target\" variables are used for the classification analysis we can do with this data set so not needed for the regression"
      ],
      "metadata": {
        "id": "zKPOL1b5n6WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most examples for modeling show how the data is **randomly** split into a training set (roughly 80% of the full dataset) and a testing set (the remaining 20% of the dataset)."
      ],
      "metadata": {
        "id": "vWLXeC8YHO2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load test train split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "VjRtwHjudDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look to see if the columsn were dropped\n",
        "num_no_nulls_df.dtypes"
      ],
      "metadata": {
        "id": "1fq7GSjEqkNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stratified sampling\n",
        "# Step 2, random sample of each DF (in this case, SEX_string == female and  SEX_string==male)\n",
        "# Format from Stackoverflow\n",
        "#larger, smaller = test_train_split(df, test_size=0.3)\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "#sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
        "X = num_no_nulls_df.drop(\"incident\",axis='columns')\n",
        "y =num_no_nulls_df[\"incident\"]\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "# for analysis on people, we would typcially want a stratified sample based on gender but that is a categorical variable and not in this example.\n",
        "# stratVar=X[\"SEX_string\"] #define the variable used for the stratified sample\n",
        "# a stratified sample ensures an equal representation of a particular category (or group) is in both the training and testin dataframes\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, train_size=.8, random_state=7, shuffle=True)#, stratify=stratVar)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "#read more\n",
        "# k fold stratification\n",
        "#https://scikit-learn.org/stable/modules/cross_validation.html#stratification"
      ],
      "metadata": {
        "id": "j4qZz7EIHQSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze the y data (the target variable) used in the training\n",
        "y_train.describe()"
      ],
      "metadata": {
        "id": "hiNqv1n3JQIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze the y data (the target variable) used in the testing\n",
        "y_test.describe()"
      ],
      "metadata": {
        "id": "h_mrLAjwd5_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at some variables with our loc and iloc skills\n",
        "X_train.iloc[1 : 13, 0 : 11]"
      ],
      "metadata": {
        "id": "2HlB2j9ff1Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# loc selects rows and columns with specific labels\n",
        "# interestingly, this won't work because the category name inserts a space into the loc statement that doesn't work\n",
        "X_train.loc[[175], [\"CP_string_asymptomatic angina\",\"trestbps\"]]\n",
        "# this cell WILL ERROR if the index rows are in the testing dataset\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MaZdxCBziB6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling sklearn"
      ],
      "metadata": {
        "id": "EZTLDSfbaEzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy already loaded in Libarary Loading section\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "JxxlmE6DgDFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()"
      ],
      "metadata": {
        "id": "2vvTFwtchnfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assess the updated X_test\n",
        "X_test.describe()"
      ],
      "metadata": {
        "id": "92czF_y461Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train)\n",
        "# you can use just this statement to combine this cell with the previous cell\n",
        "# model = LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3RrnMs0TiURT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see all the coefficents\n",
        "pd.DataFrame(model.coef_, X_train.columns, columns = ['Coeff'])"
      ],
      "metadata": {
        "id": "e7qbNpFygal_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_sq = model.score(X_train, y_train)\n",
        "print(f\"The R-squared (i.e., the coefficient of determination) is {r_sq}\")\n",
        "print(f\"intercept: {model.intercept_}\")\n",
        "#print(name, f\"coefficient: {model.coef_}\")\n",
        "print('Variance score: {}'.format(model.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "sj58IRLBiaV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make predictions with the model\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "Y4I91HucqeAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the predictions verus actual\n",
        "plt.scatter(y_test, predictions)"
      ],
      "metadata": {
        "id": "lLUnqIRgqrDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot residuals to test assumption of Linear Regression\n",
        "plt.hist(y_test - predictions)"
      ],
      "metadata": {
        "id": "UA9AW0f8qzrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## setting plot style\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "## plotting residual errors in training data\n",
        "plt.scatter(model.predict(X_train), model.predict(X_train) - y_train,\n",
        "            color = \"green\", s = 10, label = 'Train data')\n",
        "\n",
        "## plotting residual errors in test data\n",
        "plt.scatter(model.predict(X_test), model.predict(X_test) - y_test,\n",
        "            color = \"red\", s = 10, label = 'Test data')\n",
        "\n",
        "## plotting line for zero residual error\n",
        "plt.hlines(y = 0, xmin = 0, xmax = 50, linewidth = 2)\n",
        "\n",
        "## plotting legend\n",
        "plt.legend(loc = 'upper right')\n",
        "\n",
        "## plot title\n",
        "plt.title(\"Residual errors\")\n",
        "\n",
        "## method call for showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Nk5NwAWwjMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling with statsmodels.api"
      ],
      "metadata": {
        "id": "RxRa6x6gU3T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Library\n",
        "# https://www.statsmodels.org/stable/api.html#regression\n",
        "import statsmodels.api as sm\n"
      ],
      "metadata": {
        "id": "psI-S9WqVG6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if X_train has bool values, they need to be converted to numerical values of 0 or 1 for Statsmodels.api\n",
        "# If string or object variables are in the data frame, then they also need to be encoded\n",
        "# Convert boolean columns to numerical (0 and 1)\n",
        "# prompt: build a function for a df to identify bool variables and transform them to int\n",
        "\n",
        "def transform_bool_to_int(df):\n",
        "    \"\"\"\n",
        "    Identifies boolean variables in a Pandas DataFrame and transforms them to integers (1 for True, 0 for False).\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "    Returns:\n",
        "        A DataFrame with boolean columns converted to integer type.\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        if pd.api.types.is_bool_dtype(df[col]):\n",
        "            df[col] = df[col].astype(int)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "HP_NFCfC8uvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the transform_bool_to_int\n",
        "X_train = transform_bool_to_int(X_train)\n",
        "X_test = transform_bool_to_int(X_test)"
      ],
      "metadata": {
        "id": "3fjPlpNF9bIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set models\n",
        "# NOTE, this data is the same used in sklearn\n",
        "testmodel = sm.OLS(y_train, X_train)\n",
        "testmodel2 = testmodel.fit()\n",
        "print(testmodel2.summary())\n",
        "#this model summary provides the Coeffients for the Linear Regression\n",
        "# The p-value is reported in the 'P>|t|' column.\n",
        "# The p-value should be below the alpha (deault of 0.05) to be considered significant."
      ],
      "metadata": {
        "id": "R0nZX_vIVKTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling with pycaret"
      ],
      "metadata": {
        "id": "L--kinUGxB0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/pycaret/pycaret/blob/master/tutorials/Regression%20Tutorial%20Level%20Beginner%20-%20REG101.ipynb\n",
        "# https://www.pycaret.org/tutorials/html/REG102.html"
      ],
      "metadata": {
        "id": "HTIsJQUxt6NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install pycaret, for non colab notebooks, remove the ! before pip\n",
        "!pip install --upgrade pycaret # this can take about 2 minutes to complete\n",
        "import pycaret\n",
        "pycaret.__version__\n",
        "# If there is an error here, read the text box below.\n",
        "## IMPORTANT NOTE: Pycaret does not need to split the data into X_train etc.\n",
        "#   Just the df with the target variable identified is all that is needed for Pycaret."
      ],
      "metadata": {
        "id": "TF66AxS9bXOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is possible that Colab errors out with the code above. When prompted, Gemini indicated, \"To use Pycaret, you would need to switch to a Colab runtime that offers a compatible Python version (3.9, 3.10, or 3.11). You can check and change the runtime type by going to the \"Runtime\" menu at the top of the page, selecting \"Change runtime type,\" and choosing a different Python version if available.\""
      ],
      "metadata": {
        "id": "yBTrVNC4K5va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# important helper functions for pycaret in colab\n",
        "## This might not be needed anymore\n",
        "# option 1\n",
        "#from pycaret.utils import enable_colab\n",
        "#enable_colab()\n",
        "# option 2\n",
        "#from pycaret.utils import setup_colab # change it to this line.\n",
        "#setup_colab() # and change this line from enable_colab() to setup_colab() as well."
      ],
      "metadata": {
        "id": "G7D3RzqVbsH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter information at: https://www.pycaret.org/tutorials/html/REG102.html\n",
        "\n",
        "from pycaret.regression import *"
      ],
      "metadata": {
        "id": "I3oaUhQmltq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# direct copy of code cell from Pycaret tutorial\n",
        "#  using \"df\" name to define data rather than \"data\"\n",
        "df = no_nulls_df.sample(frac=0.9, random_state=786)\n",
        "data_unseen = no_nulls_df.drop(df.index)\n",
        "\n",
        "no_nulls_df.reset_index(drop=True, inplace=True)\n",
        "data_unseen.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print('Data for Modeling: ' + str(no_nulls_df.shape))\n",
        "print('Unseen Data For Predictions: ' + str(data_unseen.shape))"
      ],
      "metadata": {
        "id": "EX28WHo4bwhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the model and identify the target variable (i.e., Dependent Variable)\n",
        "# This is needed if you do ANY regression. Later you will identify specific type of regresssion or\n",
        " # use compare function to look ALL possible regression methods.\n",
        "reg01 = setup(data = df, target = 'incident', session_id=123, normalize=True, transform_target=True)\n",
        "# setting normalize=True takes care of scaling problems between ordinal and continuous variables\n",
        "\n",
        "# IMPORTANT NOTE: Pycaret will processing until you approve the automatic variable it suggests\n",
        "# scroll to the bottom of the output cell and type \"enter\" key to accept or type \"quit\" to stop"
      ],
      "metadata": {
        "id": "ogq87ZtHdLou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Builds all regression models. This is often called \"autoML\" for automatic machine learning\n",
        "# these are good baseline models\n",
        "\n",
        "#best = compare_models(exclude = ['ransac']) # exclude = ['ransac'] is from the tutorial\n",
        "compare_models()\n",
        "# This can take a little bit becasue it is building 15+ regression models.\n",
        "# Goal is low error terms (MAE, MSE, RMSE, RMSLE, MAPE) and high R2.\n",
        "# R2 ranges from zero to one.\n",
        "# R2 Values approaching zero are weak models, negative R2 means insignificant."
      ],
      "metadata": {
        "id": "SHpPaZsymMXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of the models available from Pycaret\n",
        "models() # from Pycaret Tutorial"
      ],
      "metadata": {
        "id": "i-WH7gctmNG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From Pycaret Tutorial\n",
        "lin_reg = create_model('lr')"
      ],
      "metadata": {
        "id": "ai9pxy8KmwRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify the paramaters for the lin_reg\n",
        "print(lin_reg)"
      ],
      "metadata": {
        "id": "sjrwhFARntKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning models produced above\n",
        "# This is considered iteration!\n",
        "\n",
        "'''Tuning models explained:\n",
        "\"Model tuning is also known as hyperparameter optimization.\n",
        "Hyperparameters are variables that control the training process.\n",
        "These are configuration variables that do not change during a Model training job.\n",
        "Model tuning provides optimized values for hyperparameters, which maximize your model's predictive accuracy.\"\n",
        "https://www.mlexam.com/model-tuning/\n",
        "'''\n",
        "\n",
        "tuned_lr = tune_model(lin_reg)\n",
        "#this might error out because the synthetic data did not significant models, see 'compare_models()' cell"
      ],
      "metadata": {
        "id": "OiiP75Dzn7GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify the paramaters for the tuned lin_reg\n",
        "# Compare to lin_reg\n",
        "print(tuned_lr)\n",
        "print(lin_reg)"
      ],
      "metadata": {
        "id": "KeY5ue9ToV_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing model plots for model\n",
        "plot_model(tuned_lr)"
      ],
      "metadata": {
        "id": "WSEUsSPIo3v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this allows you to assess all the features included, not just the default of the top 10\n",
        "plot_model(tuned_lr, plot='feature_all', scale = 1)"
      ],
      "metadata": {
        "id": "OEsbyoXzovk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RV14K-miVQ8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}