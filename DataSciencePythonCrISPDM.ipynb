{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNBy+DvBdbRUCbytb6DeM4o"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "❗❗❗❗ **READ EVERY Word of text** as a How-To.\n",
        "\n",
        "The python scripts (i.e., the code syntax) are good to review, but read the directions for every code cell. ❗❗❗❗"
      ],
      "metadata": {
        "id": "eiWBmVCTzVX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 1 (do before you run anything!)**\n",
        "\n",
        "Before you run any of this notebook, you should make a copy.\n",
        "\n",
        "Click \"File\" -> \"Save a copy in Drive\"\n",
        "\n",
        "Save it to your Google Drive.\n",
        "\n",
        "The default name will be\n",
        "\n",
        "`Copy of DataSciencePythonCrISPDM.ipynb`\n",
        "\n",
        "but you can name it anything you want. For class, you should use the format:\n",
        "\n",
        "*lastname_firstname_DataSciencePythonCrISPDM.ipynb*"
      ],
      "metadata": {
        "id": "MdvoRA7h5XoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ],
      "metadata": {
        "id": "-PzjPAsl0xm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Every Notebook and project should have a purpose.*\n",
        "\n",
        "The purpose of this notebook is to showcase how to apply the **Cross Industrial Stanadard Process for Data Modeling** and using Python notebooks.\n",
        "\n",
        "\n",
        "Author is Michael McCarthy (mbmccart@utica.edu)\n",
        "\n",
        "Feedback Welcomed"
      ],
      "metadata": {
        "id": "OpQdYOFs0rff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important that you update the intro to match what you are doing.\n",
        "\n",
        "If you make a copy, YOU are now the author; be sure to update that too"
      ],
      "metadata": {
        "id": "AaOo-sRT9f-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Code Cells"
      ],
      "metadata": {
        "id": "Qum38I5w4HqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Python runs code from top to bottom, left to right.\n",
        "\n",
        "✅ A python notebook like this one is the same but each *code* cell can be run independently so the order you run the cells matters."
      ],
      "metadata": {
        "id": "9fDp0Tzu176h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ There are many ways to run code cells.\n",
        "\n",
        "✅ For now, run each code cell by clicking the play button on each code cell\n",
        "or typing Shift + Enter."
      ],
      "metadata": {
        "id": "7A-ezrbuYDwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these cells in order\n",
        "# We will assign a number to a variable we will call 'number_variable'\n",
        "number_variable = 6100\n",
        "# The notebook will hold this value in memory"
      ],
      "metadata": {
        "id": "G2J2kv6n2YAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at the variable\n",
        "print(\"the number assigned to the 'number_variable' is\")\n",
        "print(number_variable)"
      ],
      "metadata": {
        "id": "wMTk8VQE2wZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The notebook will hold that value in memory until it is overwritten.\n",
        "# Let's overwrite it now\n",
        "number_variable = 22"
      ],
      "metadata": {
        "id": "pMPJd14u22Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take another look at the variable after we update it (i.e., over-write the variable)\n",
        "# NOTE that this is the same variable we assigned before: `number_variable`\n",
        "print(\"the number assigned to the 'number_variable' is \\n\" , number_variable)\n",
        "\n",
        "# NOTE: We got a similar format print output compared to code cell 3 with one print function using the `\\n` and the comma"
      ],
      "metadata": {
        "id": "vAJwOSYh3HG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The order that cells runs matters.\n",
        "# if I rerun the first cell again, I will overwrite the value again.\n",
        "# Try this now:\n",
        "# 1) Rerun the cell that has \"number_variable = 7100\"\n",
        "print(number_variable)\n",
        "# is the value 1 or 22?"
      ],
      "metadata": {
        "id": "b5cuTOid3MA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The variable is a number so we can use it in equations\n",
        "new_number_variable = number_variable + 1\n",
        "print(new_number_variable)"
      ],
      "metadata": {
        "id": "sepNlJsdmxFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Basics"
      ],
      "metadata": {
        "id": "2hCKEFnr4k-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Any line of code with a hash sign (also known as the pound sign) is a comment and will NOT as code.\n",
        "#In google Colab, comments turn the font to green.\n",
        "\"\"\" The tripple quotes, seen here are comments that carry on\n",
        "over several lines.\n",
        "The whole cell can be commented out \"\"\""
      ],
      "metadata": {
        "id": "njM9k1ax1gVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ This notebook is like a google doc but connected compute resources.\n",
        "\n",
        "✅ If the notebook is idle too long, you have to reconnect and run all the cells again from top to bottom."
      ],
      "metadata": {
        "id": "ufYLphVtM9M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indenting in Python matters\n",
        "# For example, a forward loop only works when the indenting is correct:\n",
        "for x in range(10):\n",
        "  print(x*(x+1))\n",
        "\n",
        "# A for loop will not work this without the indent on the second line\n",
        "# it will cause an error\n",
        "\"\"\"\n",
        "for x in range(10):\n",
        "print(x*(x+1))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MiE-EsI0ODyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each code cell will receive a number in the empty bracket after it is run showcasing the order in which it is run.\n",
        "# If the cell is run again, the cell number will be the most recent number (that is, the higher number)\n",
        "print(\"If you ran all the prevous code cells, this cell should have the number 9\")\n",
        "# NOTE: sometimes the cell number is hidden under the \"play\" icon.\n",
        "# To see the code cell number, move your cursor away from the play icon to reveal the number underneath."
      ],
      "metadata": {
        "id": "NOjp1sEPsvN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick Note**, instead of having 1 code cell that does 8 things, it is better to have 8 code ceslls that each do one things.\n",
        "\n",
        "This allows you to systematically build and check each step of your work."
      ],
      "metadata": {
        "id": "yDB4Wb-qAX1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libarary Loading"
      ],
      "metadata": {
        "id": "IsKKwUgt2R2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ The basic Python loaded as default into Colab is robust, but there are some extra packages (e.g., libraries) that we need to load to make sure we can do all the functions we need.\n"
      ],
      "metadata": {
        "id": "DsVTLi4_5Y49"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JMF6jYsVwsR"
      },
      "source": [
        "# Python interactive development enviroments (IDE) come with base Python 3.x but\n",
        "#     certain modules, packages, and libraries as need.\n",
        "\n",
        "# Pandas is the main way we will work with the dataframes\n",
        "# https://pandas.pydata.org/docs/getting_started/index.html#getting-started\n",
        "import pandas as pd\n",
        "# pandas defaults to not showing all rows, this pd (pandas) option update ensures all rows are shown\n",
        "# for larger datasets, update 'None' with a specific number; for now, just leav it as \"None\"\n",
        "pd.set_option('display.max_rows', None)\n",
        "# pandas defaults to not showing all columns, this pd (pandas) option update ensures all columns are shown\n",
        "pd.set_option('display.max_columns', None)\n",
        "#numpy is the \"The fundamental package for scientific computing with Python\"\n",
        "# https://numpy.org/\n",
        "import numpy as np\n",
        "# To make outputs more understandable, remove the scientific notation\n",
        "np.set_printoptions(suppress=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvOh3OwyWelW"
      },
      "source": [
        "# Packages & Libraries needed to load data from Google Drive\n",
        "# For this class, ALL Data will be loaded from Google Drive.\n",
        "# TIP: Load data just once.\n",
        "# https://pypi.org/project/PyDrive2/\n",
        "\n",
        "!pip install -U -q PyDrive2 # sometimes a library needs to be installed before it is installed\n",
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siFOuBXWWh7v"
      },
      "source": [
        "# Authenticate users to have acces to google Drive.\n",
        "# Google will make you authorize access to connect directly to the Google Drive\n",
        "# The process might change, just approve the access by approving or by clicking\n",
        "# \"Allow\", \"Continue\", and/or selecting your Utica Account (not your personal email).\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA Loading"
      ],
      "metadata": {
        "id": "SPc6WOLudFwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ To load data from Google Drive, we have to point the notebook to the specific file by ID and name. Below, we are loading example data that you will use for this notebook, but the code cell also provides the step-by-step instructions to update this data loading code cell to access any datafile in your Google Drive or shared via Google Drive."
      ],
      "metadata": {
        "id": "zyuk_K6BuSb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FOR most Utica University DSC courses assignments, data MUST be loaded from Google Drive**\n",
        "\n",
        "✅ Load data just once.\n",
        "\n",
        "✅ The file type matters: the code is a little different if you import an Excel file (xls) or a text file (txt).\n",
        "\n",
        "**NOTE:** Only faculty and students of Utica University can access the data used in this notebook without permission. **Requests from users without a utica.edu email will be declined.**"
      ],
      "metadata": {
        "id": "vwTnX2TQv7hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## You will run this cell with no updates to load the example data,\n",
        "## but here are the steps to load any CSV file from Google Drive.\n",
        "\n",
        "# 1) Open the Google Drive with the Data\n",
        "# 2) Find your assigned dataset\n",
        "# 3) Click \"share\"\n",
        "# 4) Copy link\n",
        "# 5) Paste link here:\n",
        "#    The link should look something like this: https://drive.google.com/file/d/1WVluSCNJ--RS1zqQ_0EJScPgurw9CmHj/view?usp=sharing\n",
        "# 6) Copy the unique file id, for the example above, it looks like this: 1WVluSCNJ--RS1zqQ_0EJScPgurw9CmHj\n",
        "#   Hint: the file id is all the content between the forward slashes slashes /  including the letter, numbers, dashes, and underscores\n",
        "# 7) In the next code line below, replace the unique google doc file id for the example data with the unique file id for your data.\n",
        "file_id = '1WVluSCNJ--RS1zqQ_0EJScPgurw9CmHj' # replace the id with id of file you want to access\n",
        "downloaded = drive.CreateFile({'id':file_id})\n",
        "\n",
        "# 8) Update the file name below to match the file name in the Google Drive Folder by replacing 'Heart_Synthetic.csv' with your file name.\n",
        "# Hint, you must have single or double quotes around the file name.\n",
        "file_name = 'Heart_Synthetic.csv' # Update needed here, replace the file name with the id of file you want\n",
        "downloaded.GetContentFile(file_name)\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# 9) Run all cells before this code cell (Hint: Shortcut = CTRL + F8)\n",
        "# 10) Run this cell (click the play button or Shift + Enter)\n"
      ],
      "metadata": {
        "id": "ThHAIpQV1F3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Each code Block\n",
        "In Python and coding in general, you have to test your code at each step; the fancy name for this is \"[unit testing](https://en.wikipedia.org/wiki/Unit_testing)\".\n",
        "\n",
        "You have to run the code to see if the output is what you expected.\n",
        "\n",
        "It is better to have 8 code cells doing 1 thing each than 1 code cell doing 8 things.\n"
      ],
      "metadata": {
        "id": "XL6PYFN6nwAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If there is no output, use a print() statement to generate an output.\n",
        "# Step 11 of data loading: Check the data output is what you expected.\n",
        "print(f\"{file_name} Data Shape: \",df.shape)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "5urKSCCb7tMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using prompts to build Code (i.e., [Vibe Coding](http://en.wikipedia.org/wiki/Vibe_coding/))\n",
        "\n",
        "✅ When using prompts to build code, there are several steps that **must** be completed to ensure your work is good, and you are learning what Python is doing to your data.\n",
        "\n",
        "1.   **Write a specific prompt** telling that identifies the input and expected output. You can even tell it how: Example:\n",
        "Build a swarmplot using seaborn for 'Salary', 'Position', and 'Sex' variables in df.\n",
        "2.   **Review the prompt**. Take a second to understand what is going in.\n",
        "3.   **Generate the Code.**\n",
        "4.   **Use \"Explain Code\".** Make sure you understand the code.\n",
        "5.   **Run the code cell.**\n",
        "6.   **Review the output.** Is it what you expected?\n",
        "7.   **Retain the prompt** to show where the code came from (any questions on how to document, see the course \"Coding Policy\" in the syllabus).\n",
        "\n",
        "✅ Below is an example.\n",
        "\n",
        "I entered the the prompt, now you use the \"explain code\" option. In the code cell, click the three dots to reveal \"Explain Code\", click on it.\n"
      ],
      "metadata": {
        "id": "6wjJnuHG-t9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Understanding\n",
        "\n",
        "Step 1 of the CrISP-DM process is to undertand what you are trying to do with the data.\n",
        "\n",
        "In this example notebook, we are trying to understand what variables explain patients' `incident` of heart failure"
      ],
      "metadata": {
        "id": "wxb9TuwhOsWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Understanding\n",
        "Often called \"Exploratory Data Analysis\" or simply \"EDA\""
      ],
      "metadata": {
        "id": "PKWbj3uc8G2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Once the data is loaded, we need to understand it.\n",
        "\n",
        "✅ This is the \"Data Understanding\" portion of the *Cross-Industry Standard process for Data Mining* (CrISP-DM)"
      ],
      "metadata": {
        "id": "M5U5rHOA8g8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: pandas"
      ],
      "metadata": {
        "id": "6mptpxbU1Xi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the descriptive statistics into a dataframe called \"descriptive_statistics_view\"\n",
        "# Similar to when we assigned the value of 1 to 'number_variable'\n",
        "# the equal sign is used to assign the descriptive statistics to a new dataframe\n",
        "descriptive_statistics_view = df.describe()\n"
      ],
      "metadata": {
        "id": "s5r3Uzdv73Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Because there was no output in the step above,\n",
        "# we need to review the show the descriptive_statistics_view\n",
        "descriptive_statistics_view\n"
      ],
      "metadata": {
        "id": "eGaJK-NYq52F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice how the view changes using a print statement to see the same data\n",
        "print(descriptive_statistics_view)\n"
      ],
      "metadata": {
        "id": "kt1bN-t8rT82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: tabulate"
      ],
      "metadata": {
        "id": "Ltm9djYd1toQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to improve readability and display it neatly\n",
        "# we need to import a library and configure the function\n",
        "# https://pypi.org/project/tabulate/\n",
        "import tabulate as tb"
      ],
      "metadata": {
        "id": "6u3dIzko5l_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a table for all the desciptive statistics\n",
        "print(tb.tabulate(descriptive_statistics_view, headers='keys', tablefmt='pretty'))"
      ],
      "metadata": {
        "id": "ZOMHCvQu4EES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: researchpy"
      ],
      "metadata": {
        "id": "EqsM-oBo1x_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resource: https://researchpy.readthedocs.io/en/latest/\n",
        "! pip install researchpy\n",
        "import researchpy as rp"
      ],
      "metadata": {
        "id": "mUAZqs-R3m87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Massive output that looks at all the variables\n",
        "# Note that unique identifiers (like SSN) make the report very long\n",
        "rp.codebook(df)"
      ],
      "metadata": {
        "id": "V_BUh7zp4y15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Researchpy to assess numerical and categorical variables seperately\n",
        "# https://researchpy.readthedocs.io/en/latest/\n",
        "\n",
        "print(\"\\n--- Descriptive Statistics for Continuous Variables ---\")\n",
        "print(rp.summary_cont(df.select_dtypes(include=np.number)))\n",
        "\n",
        "print(\"\\n--- Descriptive Statistics for Categorical Variables ---\")\n",
        "print(rp.summary_cat(df.select_dtypes(exclude=np.number)))\n",
        "# for the Heart Data, you can see which variable is unique for each record\n",
        "# HINT, look at `SSN`)"
      ],
      "metadata": {
        "id": "MN2JvGxazdqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: review df for a list of multiple variable names from a list and drops the variables from the df. the defualt list is just one variable 'RecordID'\n",
        "# NOTE, this was my third prompt. The first two were so vague that Gemini had a **very** long function.\n",
        "\"\"\"\n",
        "LLMs, like Gemini, will often build a function as part of the code it generates.\n",
        "Python uses keyword 'def' to define a function. These are also called a \"reserved\" word.\n",
        "This is great because a function is reusable within the notebook or can be brought over to another notebook.\n",
        "It is important that the function is run, not just defined.\n",
        "'def' just defines the function, it must be called using the function name.\n",
        "Running the function is best done in a seperate cell after the function is defined.\n",
        "\"\"\"\n",
        "def drop_variables(df, variables_to_drop=['RecordID']):\n",
        "    \"\"\"\n",
        "    Reviews a Pandas DataFrame for a list of variable names and drops them.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame.\n",
        "        variables_to_drop: A list of variable names to drop. Defaults to ['RecordID'].\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with the specified variables removed, or the original DataFrame if no variables are found.\n",
        "        Prints a message indicating which variables were dropped or if none were found.\n",
        "    \"\"\"\n",
        "\n",
        "    variables_dropped = []             # defines an empty list to add dropped variables to\n",
        "    for var in variables_to_drop:      # for loop to look at each varible in the list\n",
        "        if var in df.columns:          # if test to look if the variable name is in the dataframe's columns\n",
        "            df = df.drop(var, axis=1)  # drop the variable from the dataframe\n",
        "            variables_dropped.append(var)  # add the variable to the list of dropped variables\n",
        "\n",
        "    if variables_dropped:   # if test defaults to \"TRUE\" so if the list is not blank, it will do the rest of the if statement\n",
        "        print(f\"Variables dropped: {variables_dropped}\")\n",
        "    else:                   # else statements are optional, but should be used\n",
        "        print(\"No variables to drop found in the DataFrame.\")\n",
        "\n",
        "    # Final Notes:\n",
        "    \"\"\"\n",
        "    Gemini did not identify the much simplier way to do this.\n",
        "    However, this one line of coade does not build or report the variables_dropped list.\n",
        "    \"\"\"\n",
        "    # df = df.drop(variables_to_drop, errors= 'ignore', axis=1)\n",
        "    \"\"\" This is how it would look in a production enviornment, to prvent the extra memory needed to overwrite the df. \"\"\"\n",
        "    # df.drop(columns=variables_to_drop, errors='ignore', inplace=True) # `axis=1` indicates columns and removed because it is the default, therefore not needed\n",
        "\n",
        "    return df               # identifies that the fucntion returns the original df modified\n"
      ],
      "metadata": {
        "id": "ne6B8p_dAvNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the fution just defined above, it needs to be run to apply it to the data.\n",
        "# To run for a different pdateset, just update variables in the list\n",
        "# in this case, we are removing numerical that act as identification variables for patients.\n",
        "df = drop_variables(df, variables_to_drop=['RecordID', 'SSN']) # SSN was added to remove list due to privacy concerns"
      ],
      "metadata": {
        "id": "V07-pWJsCC4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assess the data after the drop and look at the data types.\n",
        "# Make sure we have numbers, not just strings because a CSV can often load all variables as strings.\n",
        "print(\"\\nTraining DATA\\n\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "NlINKSX82ktW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the dataframe to show the variables and open Interactive table in Colab\n",
        "# NOTE, if you scroll all the way to the right, click the Table icon to view an \"interactive sheet\" that acts like an excel spreadsheet\n",
        "# The plot icon will generate many suggested plots, only some of them are worth using in your analysis.\n",
        "df\n",
        "# After you you run and view your df, you should see the \"Next steps:\" options under the datframe output. These are wise to use for the very first steps of your EDA.\n",
        "# After you you \"View recommended plots\", be sure to dig deeper with your own plots"
      ],
      "metadata": {
        "id": "FQhsPck6Epq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is possible to make new variables with math, this is called \"feature engineering\"\n",
        "# in this case, I will make a new variable by dividing one variable by another.\n",
        "# because there could be multiple dataframes with the same variable name, I must\n",
        "# identifiy the df AND the variable name in brackets and quotes.\n",
        "\n",
        "# In python, the equal sign is used to assinge a variable, in this case \"risk\".\n",
        "df[\"risk\"] = (df[\"thalach\"] + df[\"chol\"])\n",
        "# These variables are unique to the example dataset in \"Heart_Synthetic.csv\", update, move, or delete this cell or it will error because\n",
        "# the variables are not in the new dataset (i.e., dataframe or df)"
      ],
      "metadata": {
        "id": "BOA12-Rg41yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review the results of the new variable.\n",
        "df[\"risk\"].describe()\n",
        "# This variable is unique to the example dataset in \"Heart_Synthetic.csv\", update, move, or delete this cell or it will error because\n",
        "# the variables are not in the new dataset (i.e., dataframe or df)"
      ],
      "metadata": {
        "id": "7NyS7iP4ksky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: Visualizations: matplotlib"
      ],
      "metadata": {
        "id": "v21LcPZ0wRKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Humans are very visual. We can often see patterns in data visualizations that we would not see in descriptive statestics tables or looking at the raw data."
      ],
      "metadata": {
        "id": "PJO0t4LgDihs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MatPlotLib is a common visulaization package.\n",
        "# note that just pyplot is added in, not the full package\n",
        "from matplotlib import pyplot as plt\n",
        "#It is a function that renders the figure in a notebook (instead of displaying a dump of the figure object).\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "1weHVpfC4-v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can build historgram plots for variables one by one using matplotlib\n",
        "df.hist('risk')\n",
        "df.hist('trestbps')\n",
        "# But this can be vary tidious.\n",
        "# These variables are unique to the example dataset in \"Heart_Synthetic.csv\".\n",
        "# To update this code for other datasets, the variable names must be udpated.\n",
        "# if you load data, you must move, or delete this cell or it will error because\n",
        "# the variables are not in the new dataset (i.e., dataframe or df)"
      ],
      "metadata": {
        "id": "S1kYWsfUFH-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## WE can use python to build all the appropriate histograms.\n",
        "# prompt: build histograms for all numerical values in df\n",
        "\n",
        "# Iterate over numerical columns and create histograms\n",
        "for col in df.select_dtypes(include=np.number):\n",
        "  plt.figure()  # Create a new figure for each histogram\n",
        "  plt.hist(df[col], bins=10)  # Adjust the number of bins as needed\n",
        "  plt.title(f'Histogram of {col}')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "rQ2dvGxB8Nlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The histograms did not look right with gaps in the x axis so I updated the prompt\n",
        "# prompt: build a histogram for all numerical variables in the df. set the number of bins to the number of unique values in the variable\n",
        "\"\"\"\n",
        "import pandas as pd # Gemini will include the packages you need even though you already imported these in the first code cell.\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import pyplot as plt\n",
        "\"\"\"\n",
        "\n",
        "# Assuming 'df' is already loaded as in your provided code\n",
        "\n",
        "for col in df.select_dtypes(include=['number']):\n",
        "  num_unique = df[col].nunique()\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.hist(df[col], bins=num_unique)\n",
        "  plt.title(f'Histogram of {col}')\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "cBirCYaFHNuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: Visualizations: seaborn"
      ],
      "metadata": {
        "id": "rx1t84ubHL46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ *Seaborn* is a great open-sourse tool and generally more user-friendly than *matplotlip*."
      ],
      "metadata": {
        "id": "P7lC5ebZFbGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# need the seaborn library loaded.\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "6Mfm6b2lHR1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the same histogram with seaborn library\n",
        "#you have lots of options\n",
        "sns.histplot(data=df, x='risk')\n",
        "# This variable is unique to the example dataset in \"Heart_Synthetic.csv\", update, move, or delete this cell or it will error because\n",
        "# the variables are not in the new dataset (i.e., dataframe or df)"
      ],
      "metadata": {
        "id": "G7Y2Zx4-HWXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://seaborn.pydata.org/generated/seaborn.catplot.html\n",
        "sns.catplot(data=df, x=\"THAL_string\", y=\"risk\")\n",
        "# This variable is unique to the example dataset in \"Heart_Synthetic.csv\", update, move, or delete this cell or it will error because\n",
        "# the variable is not in the new dataset (i.e., dataframe or df)"
      ],
      "metadata": {
        "id": "wHFBXGSzJYoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#same data, but with a violin plot\n",
        "#https://seaborn.pydata.org/generated/seaborn.violinplot.html#seaborn.violinplot\n",
        "sns.violinplot(data=df, x=\"THAL_string\", y=\"risk\")"
      ],
      "metadata": {
        "id": "Veo0L4eDJ_yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#swarm Plot\n",
        "#https://seaborn.pydata.org/generated/seaborn.swarmplot.html#seaborn.swarmplot\n",
        "sns.catplot(data=df, x=\"THAL_string\", y=\"risk\", hue=\"SEX_string\", kind=\"swarm\")\n"
      ],
      "metadata": {
        "id": "WzxjY5vKKSPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Box & Whisker\n",
        "#https://seaborn.pydata.org/generated/seaborn.boxplot.html#seaborn.boxplot\n",
        "sns.boxplot(data=df, x=\"THAL_string\", y=\"risk\", hue=\"SEX_string\")"
      ],
      "metadata": {
        "id": "UZoxihHDK6YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: insert sns correlation matrix for all the numerical variables\n",
        "\n",
        "# Correlation Matrix\n",
        "# https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "correlation_matrix = df.select_dtypes(include=np.number).corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T82Re6YcDwua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a loop to identify all numerical values in the df and then perform a boxplot and a histogram. Be sure to label each plot.\n",
        "\n",
        "# Loop through columns to identify numerical features\n",
        "for col in df.columns:\n",
        "  if pd.api.types.is_numeric_dtype(df[col]):\n",
        "    # Create a boxplot\n",
        "    plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.show()\n",
        "\n",
        "    # Create a histogram\n",
        "    plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "    sns.histplot(df[col])\n",
        "    plt.title(f'Histogram of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0ma-FxaGLNgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: Visualizations: plotly"
      ],
      "metadata": {
        "id": "0_wIphIaHnig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ *Plotly* is just another visualization library that provides other options."
      ],
      "metadata": {
        "id": "Td4KBhFqFWG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://plotly.com/python/plotly-express/\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "8qKclJOgyKte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Look at the Dependent Variable as it relates to these categorical variables\n",
        "#consider this type of side-by-side box and whisker\n",
        "\n",
        "px.box(data_frame=df,x='SEX_string', y='incident')\n"
      ],
      "metadata": {
        "id": "I1rqzxj66EIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "px.box(data_frame=df,x='Race_String', y='incident')\n",
        "# does the plot show outliers????"
      ],
      "metadata": {
        "id": "7GPeS9J5INSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: query() in Pandas"
      ],
      "metadata": {
        "id": "Vhu2_aXdXrSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Queries allow for deeper data investivgation as part of your EDA."
      ],
      "metadata": {
        "id": "SIYCUN4UGyU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VERY IMPORTANT\n",
        "# Column names can not have spaces for Pandas Querry\n",
        "# This code cell is important to run even though you think your varaibles names already have the spaces removed.\n",
        "\n",
        "# Use print function to output and assess the old column names\n",
        "print(\"Old Names: \\n\",  df.dtypes)\n",
        "\n",
        "# replace the spaces with underscores\n",
        "df.columns=[x.replace(\" \", \"_\") for x in df.columns]\n",
        "\n",
        "# Use print function to output and assess the new column names:\n",
        "print(\"New Column names:\\n\", df.dtypes) #look at new column names"
      ],
      "metadata": {
        "id": "3Ccv5wIu_mxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important NOTE:** The names of the variable names are changed from this point on."
      ],
      "metadata": {
        "id": "_7vfsR7WySPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A pandas query is writen as a long string.\n",
        "\"\"\"\n",
        "It is important that double quotes are used to define the querry and\n",
        "single quotes used to identify categories (e.g., 'male' below)\n",
        "\"\"\"\n",
        "df.query(\"age > 65 and \tSEX_string == 'male'\", inplace=False)"
      ],
      "metadata": {
        "id": "AMAn_K9BL-ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#column names can not have spaces for Pandas Query\n",
        "#dfTrain.columns=[x.replace(\" \", \"_\") for x in dfTrain.columns]\n",
        "# using the and to make a multiple criteria\n",
        "#if pointing to categorical data, the criteria should be in single quotes\n",
        "\n",
        "df.query(\"age > 45 and ca>0 and RESTECG_string=='normal'\", inplace=False)\n",
        "# The results should be only the portion of the data that meets the criteria in the query."
      ],
      "metadata": {
        "id": "FVb4FOn-DWsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same query as above but with \"or\" satement with bar |\n",
        "#same query but with or statements\n",
        "df.query(\"age > 45 | \tca>0 |RESTECG_string=='normal'\",inplace=False)"
      ],
      "metadata": {
        "id": "dKIVU1mBOagD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same query as bove but with describe()\n",
        "df.query(\"age > 45 | \tca>0 |RESTECG_string=='normal'\",inplace=False).describe()"
      ],
      "metadata": {
        "id": "ITGHR_vkh7TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.query(\"ca != 0 and SEX_string > 'female' \").describe()"
      ],
      "metadata": {
        "id": "1tYn5zg5U89A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This shows just the descriptive statistics ( with .describe() ) for two variables (after \".filter\") for data that matches the query\n",
        "print(df.query(\"ca != 0\").filter(['age', 'FBS_string']).describe())"
      ],
      "metadata": {
        "id": "3Rr_hpEWeUYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is possible to define a new dataframe with the equals sign with the query criteria\n",
        "new_criteria = 1.3\n",
        "df_filter=df.query(\"oldpeak > @new_criteria\")\n",
        "# print(df_filter.head)\n",
        "print(df_filter.shape)\n",
        "# There are benefits to having a new dataframe but there are challenges too.\n",
        "# Can you think of some pros and cons?"
      ],
      "metadata": {
        "id": "vljZXX2RLPEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive Statistics for query results\n",
        "# https://www.youtube.com/watch?v=mBZwYUaIRfY\n",
        "df_filter.describe()"
      ],
      "metadata": {
        "id": "STEhgYKjhG_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useing a variable to define criteria\n",
        "new_criteria=1.4\n",
        "# use the \"@\" to designate the variable in the string query\n",
        "df.query(\"oldpeak > @new_criteria\")"
      ],
      "metadata": {
        "id": "kzjC_3jLJ6IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: filter in Pandas"
      ],
      "metadata": {
        "id": "TZ0yjpFIRTbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html?highlight=filter#pandas.DataFrame.filter\n",
        "# filter string (object) data if it has a certain string in it.\n",
        "df['THAL_string'].str.contains('defect')\n",
        "#returns a boolean Ture or False\n"
      ],
      "metadata": {
        "id": "31Q68AnSFgPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: loc & iloc in Pandas"
      ],
      "metadata": {
        "id": "UPTPwQy0I72D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loc selects rows and columns with specific labels\n",
        "# iloc selcts rows and columns by index\n",
        "# to find particular rows , colums\n",
        "df.iloc[0:12,10:12]\n",
        "#the iloc will return the first 13 items from the filtered df\n",
        "#EXANG_string\tFBS_string are columns 10 & 11 (the range is exclusive on the high side meaning that 12 is not include)\n",
        "#if you want column 12, update to \"10:13\""
      ],
      "metadata": {
        "id": "OPKFuJdJiOW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loc for the same\n",
        "df.loc[:12,['EXANG_string','FBS_string']]\n",
        "#the loc will return the first items in index 0-12 from the filtered df\n",
        "# remember, df_filter has rows that were filtered away.\n",
        "# the loc and iloc would be the same on the orginal df"
      ],
      "metadata": {
        "id": "GrpKCvLMSyyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:12,['EXANG_string','FBS_string']]"
      ],
      "metadata": {
        "id": "kRBcFxodTwTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0:12,10:12]"
      ],
      "metadata": {
        "id": "K45zd_NJTyvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are use iloc to look through the filtered data frame\n",
        "df_filter.iloc[0:12] #only looking through index 0-12 (the filtered data only has a few outputs for this range)\n",
        "# NOTE in the loc statement, the [0:12] above is the same as [:12] used in the next cell below"
      ],
      "metadata": {
        "id": "Wlvw_Ixdv82s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the Full dataframe (unfiltered) has all the values in indices 1-12\n",
        "df.iloc[:12]"
      ],
      "metadata": {
        "id": "IQSyYo8mPrwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic feature engineering, adding a new variable\n",
        "df[\"year_born\"]= 2022-df[\"age\"]\n",
        "print(df[\"year_born\"].head(10))"
      ],
      "metadata": {
        "id": "GGVqZ5T7juXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TWO ways to build dataframes based on two ways to search data loc and Query()\n",
        "oldMen_query = df.query(\"SEX_string == 'male' and age > 65\", inplace=False)\n",
        "oldMen_loc = df.loc[(df.SEX_string == 'male') & (df.age > 65 )]\n",
        "\n",
        "# Test the output\n",
        "print(oldMen_query.shape)\n",
        "print(oldMen_loc.shape)\n"
      ],
      "metadata": {
        "id": "DxhsUQVdbL0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#should be the same numbers for the oldMen_loc and oldMen_query\n",
        "oldMen_loc.describe()"
      ],
      "metadata": {
        "id": "9ryHG8UBhMtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oldMen_query.describe()"
      ],
      "metadata": {
        "id": "zF8xUFigl9FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "Also called \"Data Wrangling\""
      ],
      "metadata": {
        "id": "ntjuCGqVc9R8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ The data can be very \"dirty\" or not in the form we need it to be so we do considerable data wrangling.\n",
        "\n",
        "✅ This is the \"Data Preparation\" step in the CriSP-DM."
      ],
      "metadata": {
        "id": "4dNdZaKsAkWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data prep for modeling"
      ],
      "metadata": {
        "id": "4igAPa_xaaFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assess the shape before get dummies\n",
        "df.shape"
      ],
      "metadata": {
        "id": "UR3x9xDgcctF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load variable name to paste into get dummies below\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "REI2qQ10a973"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing nulls and nans"
      ],
      "metadata": {
        "id": "w_4oAjz68Q7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: build a new dataframe called \"no_nulls_df\" that will apply the mean to each null in the numerical variables and the mode to each null in the categorical variables.\n",
        "\n",
        "# Create a copy of the DataFrame to avoid modifying the original\n",
        "no_nulls_df = df.copy()\n",
        "\n",
        "# Fill nulls in numerical columns with the mean\n",
        "numerical_cols = no_nulls_df.select_dtypes(include=np.number).columns\n",
        "for col in numerical_cols:\n",
        "    no_nulls_df[col] = no_nulls_df[col].fillna(no_nulls_df[col].mean())\n",
        "\n",
        "# Fill nulls in categorical columns with the mode\n",
        "categorical_cols = no_nulls_df.select_dtypes(exclude=np.number).columns\n",
        "for col in categorical_cols:\n",
        "    no_nulls_df[col] = no_nulls_df[col].fillna(no_nulls_df[col].mode()[0])\n"
      ],
      "metadata": {
        "id": "69ZJiSm3HDvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the synthetic heart data, note that the dfshape with get_dummies went from 17 columns to 32 columns\n",
        "no_nulls_df.shape"
      ],
      "metadata": {
        "id": "cQVhjrCWbj6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing Categorical Variables"
      ],
      "metadata": {
        "id": "JEbwuxNi8KqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*sklearn* will not model with categorical variables.\n",
        "\n",
        "*sklearn* has its own encoding tools that transform variables into 0 representing  \"False\" and 1 representing \"True\". This is the better option."
      ],
      "metadata": {
        "id": "nvbFvt608Ygm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need to understand which variables are categorical.\n",
        "# prompt: identify all the unique values in each categorical variable, test for nominal varirables by counting the unique values and if they are with 50% of the total number or data rows,  remove the variable from print statement\n",
        "\n",
        "# Identify nominal variables and filter based on unique value counts\n",
        "for col in df.select_dtypes(include=['object']):\n",
        "    unique_counts = df[col].nunique()\n",
        "    if unique_counts <= 0.5 * len(df):  # Check if unique values are within 50% of total rows\n",
        "        print(f\"Unique values for {col}:\")\n",
        "        print(df[col].unique())\n",
        "        print(\"-\" * 20)\n",
        "    else:\n",
        "        print(f\"Variable '{col}' has too many unique values ({unique_counts}) to be considered nominal and will be excluded from detailed analysis\")\n"
      ],
      "metadata": {
        "id": "JAFwQaQppJ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ordinal Encoding"
      ],
      "metadata": {
        "id": "Q2_gcVYwFeJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mappings for ordinal variables\n",
        "# Ordinal variable have structure and *should* be treated differently than nominal variables.\n",
        "SLOPE_string_mapping = {'upsloping': 1, 'downsloping': -1, 'flat': 0}\n",
        "\n",
        "# Before applying the mapping, check if the columns exist in no_nulls_df\n",
        "if 'SLOPE_string' in no_nulls_df.columns:\n",
        "    no_nulls_df['ExterQual_numeric'] = no_nulls_df['SLOPE_string'].map(SLOPE_string_mapping)\n",
        "else:\n",
        "    print(\"Warning: 'SLOPE_string' column not found in the DataFrame.\")\n",
        "# Check if the variable unique values are updated?\n",
        "print(no_nulls_df[\"ExterQual_numeric\"].unique())"
      ],
      "metadata": {
        "id": "QFjcjC-76KJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the oridnal data was updated, drop the string variable\n",
        "# prompt: drop SLOPE_string from df\n",
        "\n",
        "if 'SLOPE_string' in no_nulls_df.columns:\n",
        "    print(\"Data Frame shape before drop \", no_nulls_df.shape[1]) # the shape reports rows and columns, we want just the colunns, that is why we index it at 1\n",
        "    no_nulls_df = no_nulls_df.drop('SLOPE_string', axis=1)  # axis defines rows or columns: 1 = colunmns.\n",
        "    print(\"'SLOPE_string' column dropped successfully.\")\n",
        "    print(\"Data Frame shape after drop \", no_nulls_df.shape[1])\n",
        "else:\n",
        "    print(\"Warning: 'SLOPE_string' column not found in the DataFrame.\")"
      ],
      "metadata": {
        "id": "Jv1ESSwv7F87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nominal Encoding"
      ],
      "metadata": {
        "id": "mBmqkM66H8Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sklearn for categorical encoding and train_test_split\n",
        "# https://www.freecodecamp.org/news/how-to-build-and-train-linear-and-logistic-regression-ml-models-in-python/\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "IoayUBHpJeYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
        "# interesting article about why one-hot encoding (ohe) is better in ML\n",
        "# https://albertum.medium.com/preprocessing-onehotencoder-vs-pandas-get-dummies-3de1f3d77dcc\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Identify a list of nominal variables (that is, variables with 3 or more categories)\n",
        "nominal_variable_name=[\"CP_string\", \"RESTECG_string\",\"THAL_string\",\"Race_String\"]\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False) # Set sparse_output to False to get a dense array\n",
        "\n",
        "ohe_transformed = ohe.fit_transform(no_nulls_df[nominal_variable_name])\n",
        "# Get feature names for the new columns\n",
        "ohe_column_names = ohe.get_feature_names_out(nominal_variable_name)\n",
        "\n",
        "# Convert to DataFrame\n",
        "ohe_df = pd.DataFrame(ohe_transformed, columns=ohe_column_names, index=no_nulls_df.index)\n",
        "\n",
        "print(ohe_df.head(2))\n"
      ],
      "metadata": {
        "id": "pM4mXFzibjFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dummy Encoding"
      ],
      "metadata": {
        "id": "muVSg12sIGhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify a list of dummy variables (that is, variables with 2 categories)\n",
        "dummy_variables_names = ['SEX_string', 'EXANG_string',\"FBS_string\"]\n",
        "\n",
        "# For dummy variables, we can drop one category to avoid multicollinearity (if not handled by OHE directly)\n",
        "dummy = OneHotEncoder(drop='first',  # 'first' drops the first category, needed for dichomomous variables\n",
        "                      sparse_output=False)\n",
        "\n",
        "dummy_transformed = dummy.fit_transform(no_nulls_df[dummy_variables_names])\n",
        "# Get feature names for the new columns\n",
        "dummy_column_names = dummy.get_feature_names_out(dummy_variables_names)\n",
        "\n",
        "# Convert to DataFrame\n",
        "dummy_df = pd.DataFrame(dummy_transformed, columns=dummy_column_names, index=no_nulls_df.index)\n",
        "\n",
        "print(dummy_df.head(2))\n",
        "\n"
      ],
      "metadata": {
        "id": "uDWiiqHmDBNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, combine all encoded variables with no_nulls_df and drop the original categorical columns\n",
        "\n",
        "# Concatenate the original df (without the original categorical columns) with the new encoded dfs\n",
        "list_categorical_cols_removed = nominal_variable_name + dummy_variables_names\n",
        "\n",
        "# Drop the original categorical columns first\n",
        "no_nulls_df = no_nulls_df.drop(columns=list_categorical_cols_removed)\n",
        "\n",
        "# Bring all the dataframes together with concatenate\n",
        "no_nulls_df = pd.concat([no_nulls_df, ohe_df, dummy_df], axis=1)\n",
        "\n",
        "print(\"\\nShape of no_nulls_df after adding encoded variables:\", no_nulls_df.shape)\n",
        "print(\"\\nFirst 5 rows of no_nulls_df with encoded variables:\")\n",
        "print(no_nulls_df.head(2))"
      ],
      "metadata": {
        "id": "mG7QgmMUI7R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most examples for modeling show how the data is **randomly** split into a training set (roughly 80% of the full dataset) and a testing set (the remaining 20% of the dataset)."
      ],
      "metadata": {
        "id": "vWLXeC8YHO2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify and drop the TARGET_string column as it's a categorical target variable\n",
        "# and is not needed for the independent variables (X).\n",
        "# If 'TARGET_string' was intended as a numerical target for regression, it would be 'y'.\n",
        "if 'TARGET_string' in no_nulls_df.columns:\n",
        "    num_no_nulls_df = no_nulls_df.drop(columns=['TARGET_string'])\n",
        "else:\n",
        "    num_no_nulls_df = no_nulls_df.copy()\n",
        "    print(\"Warning: 'TARGET_string' column not found, proceeding with existing DataFrame.\")\n",
        "\n",
        "#look to see if the columns were dropped and inspect dtypes\n",
        "num_no_nulls_df.dtypes"
      ],
      "metadata": {
        "id": "1fq7GSjEqkNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling: statsmodels.api"
      ],
      "metadata": {
        "id": "RxRa6x6gU3T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the library to split the data\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "VjRtwHjudDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test_split is part of 'Modeling' (not data preparation)\n",
        "# Format from Stackoverflow\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "X = num_no_nulls_df.drop(\"incident\",axis='columns')\n",
        "y =num_no_nulls_df[\"incident\"]\n",
        "print(y.shape)\n",
        "print(X.shape)\n",
        "# for analysis on people, we would typically want a stratified sample based on gender but that is a categorical variable and not in this example.\n",
        "# stratVar=X[\"SEX_string\"] #define the variable used for the stratified sample\n",
        "# a stratified sample ensures an equal representation of a particular category (or group) is in both the training and testin dataframes\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, train_size=.8, random_state=7, shuffle=True)#, stratify=stratVar)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "#read more\n",
        "# k fold stratification\n",
        "#https://scikit-learn.org/stable/modules/cross_validation.html#stratification"
      ],
      "metadata": {
        "id": "j4qZz7EIHQSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you know EXACTLY which variables are going into your model."
      ],
      "metadata": {
        "id": "JGamCpvvL4Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze the y data (the target variable) used in the training\n",
        "y_train.describe()"
      ],
      "metadata": {
        "id": "hiNqv1n3JQIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze the y data (the target variable) used in the testing\n",
        "y_test.describe()"
      ],
      "metadata": {
        "id": "h_mrLAjwd5_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.describe()"
      ],
      "metadata": {
        "id": "LmEJYNTYMHLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.describe()"
      ],
      "metadata": {
        "id": "m6fDb6gqMN-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load modeling Library\n",
        "# https://www.statsmodels.org/stable/api.html#regression\n",
        "import statsmodels.api as sm\n"
      ],
      "metadata": {
        "id": "psI-S9WqVG6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelset models\n",
        "# make the function\n",
        "testmodel = sm.OLS(y_train, X_train)\n",
        "\n",
        "# Use the fit to run the model\n",
        "model = testmodel.fit()\n",
        "print(model.summary())\n",
        "#this model summary provides the Coeffients for the Linear Regression\n",
        "# The p-value is reported in the 'P>|t|' column.\n",
        "# The p-value should be below the alpha (deault of 0.5) to be considered significant."
      ],
      "metadata": {
        "id": "R0nZX_vIVKTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When you understand this notebook come to this section."
      ],
      "metadata": {
        "id": "whXiewnCbQLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy and pasted this output into your quiz!\n",
        "print(file_id)"
      ],
      "metadata": {
        "id": "Ech4kRPTbgWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}